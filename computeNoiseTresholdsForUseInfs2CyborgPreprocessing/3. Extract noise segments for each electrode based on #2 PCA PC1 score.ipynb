{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 25:=============================================>          (13 + 3) / 16]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dataPath = data/2_MEA2_raw.csv\n",
       "experimentName = #2\n",
       "header = Array(TimeStamp [µs], 47 (ID=0) [pV], 48 (ID=1) [pV], 46 (ID=2) [pV], 45 (ID=3) [pV], 38 (ID=4) [pV], 37 (ID=5) [pV], 28 (ID=6) [pV], 36 (ID=7) [pV], 27 (ID=8) [pV], 17 (ID=9) [pV], 26 (ID=10) [pV], 16 (ID=11) [pV], 35 (ID=12) [pV], 25 (ID=13) [pV], Ref (ID=14) [pV], 14 (ID=15) [pV], 24 (ID=16) [pV], 34 (ID=17) [pV], 13 (ID=18) [pV], 23 (ID=19) [pV], 12 (ID=20) [pV], 22 (ID=21) [pV], 33 (ID=22) [pV], 21 (ID=23) [pV], 32 (ID=24) [pV], 31 (ID=25) [pV], 44 (ID=26) [pV], 43 (ID=27) [pV], 41 (ID=28) [pV], 42 (ID=29) [pV], 52 (ID=30) [pV], 51 (ID=31) [pV], 53 (ID=32) [pV], 54 (ID=33) [pV], 61 (ID=34) [pV], 62 (ID=35) [pV], 71 (ID=36) [pV], 63 (ID=37) [pV], 72 (ID=38) [pV], 82 (ID=39) [pV], 73 (ID=40) [p...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[TimeStamp [µs], 47 (ID=0) [pV], 48 (ID=1) [pV], 46 (ID=2) [pV], 45 (ID=3) [pV], 38 (ID=4) [pV], 37 (ID=5) [pV], 28 (ID=6) [pV], 36 (ID=7) [pV], 27 (ID=8) [pV], 17 (ID=9) [pV], 26 (ID=10) [pV], 16 (ID=11) [pV], 35 (ID=12) [pV], 25 (ID=13) [pV], Ref (ID=14) [pV], 14 (ID=15) [pV], 24 (ID=16) [pV], 34 (ID=17) [pV], 13 (ID=18) [pV], 23 (ID=19) [pV], 12 (ID=20) [pV], 22 (ID=21) [pV], 33 (ID=22) [pV], 21 (ID=23) [pV], 32 (ID=24) [pV], 31 (ID=25) [pV], 44 (ID=26) [pV], 43 (ID=27) [pV], 41 (ID=28) [pV], 42 (ID=29) [pV], 52 (ID=30) [pV], 51 (ID=31) [pV], 53 (ID=32) [pV], 54 (ID=33) [pV], 61 (ID=34) [pV], 62 (ID=35) [pV], 71 (ID=36) [pV], 63 (ID=37) [pV], 72 (ID=38) [pV], 82 (ID=39) [pV], 73 (ID=40) [pV], 83 (ID=41) [pV], 64 (ID=42) [pV], 74 (ID=43) [pV], 84 (ID=44) [pV], 85 (ID=45) [pV], 75 (ID=46) [pV], 65 (ID=47) [pV], 86 (ID=48) [pV], 76 (ID=49) [pV], 87 (ID=50) [pV], 77 (ID=51) [pV], 66 (ID=52) [pV], 78 (ID=53) [pV], 67 (ID=54) [pV], 68 (ID=55) [pV], 55 (ID=56) [pV], 56 (ID=57) [pV], 58 (ID=58) [pV], 57 (ID=59) [pV]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// the csv file is a direct ASCII export form\n",
    "// Multi Channel DataManager for a given MEA.\n",
    "// In MultiChannel DataManager, specify ASCII export\n",
    "// to prepare the file for this notebook.\n",
    "val dataPath = \"data/2_MEA2_raw.csv\"\n",
    "val experimentName = \"#2\"\n",
    "// ---- Get header as a string Array.\n",
    "val header = spark\n",
    "    .read.textFile(dataPath)\n",
    "    .filter(_.contains(\"TimeStamp\"))\n",
    "    .flatMap(_.split(\",\"))\n",
    "    .collect\n",
    "// ---- Load the data into a DataFrame\n",
    "// with DataType as Int .\n",
    "// DataFrame is the same as a Dataset with \n",
    "// import org.apache.spark.sql.Row\n",
    "// as the rows.\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types._\n",
    "// Pre-define how the data layout looks like.\n",
    "// This leads to skipping of the first lines\n",
    "// in the raw csv that do not match to the schema,\n",
    "// if the read command is combined with the option\n",
    "// DROPMALFORMED.\n",
    "val schema = StructType(for (e <- header) yield {StructField(e, IntegerType, true)})\n",
    "// load the data.\n",
    "lazy val data = spark.read\n",
    "    .schema(schema)\n",
    "    .option(\"mode\", \"DROPMALFORMED\")\n",
    "    .csv(dataPath)\n",
    "// load the noise segments from #2 SVD score1\n",
    "// load the data\n",
    "val noiseDataPath = \"data/#2_PC1_score_noise_segments/part-00000\"\n",
    "val noiseSegmentsSchema = StructType(\n",
    "    StructField(\"Noise\", DoubleType, true) :: \n",
    "    StructField(\"Index\", IntegerType, true) :: Nil)\n",
    "val noiseSegmentsRDD = spark.read\n",
    "    .textFile(noiseDataPath)\n",
    "    .map(_.replaceAllLiterally(\"(\",\"\"))\n",
    "    .map(_.replaceAllLiterally(\")\",\"\"))\n",
    "    .map(_.split(\",\"))\n",
    "    .rdd\n",
    "    .map(r => Row(r(0).toDouble, r(1).toInt))\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "val noiseSegmentsDF = sqlContext.createDataFrame(noiseSegmentsRDD, noiseSegmentsSchema)\n",
    "val timeStampAndIndexSchema = StructType(\n",
    "    StructField(\"TimeStamp [µs]\", IntegerType, true) :: \n",
    "    StructField(\"Index\", IntegerType, true) :: Nil)\n",
    "val timeStampAndIndexRDD = data.select(\"TimeStamp [µs]\")\n",
    "    .rdd\n",
    "    .zipWithIndex\n",
    "    .map(r => Row(r._1.getAs[Int](0), r._2.toInt))\n",
    "val timeStampAndIndexDF = sqlContext.createDataFrame(timeStampAndIndexRDD, timeStampAndIndexSchema)\n",
    "val noiseSegmentsTimeStamps = timeStampAndIndexDF\n",
    "    .join(noiseSegmentsDF, timeStampAndIndexDF(\"Index\") === noiseSegmentsDF(\"Index\"))\n",
    "    .select(\"TimeStamp [µs]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataNoiseSegments = [TimeStamp [µs]: int, 47 (ID=0) [pV]: int ... 59 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[TimeStamp [µs]: int, 47 (ID=0) [pV]: int ... 59 more fields]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// extract the noise segments for all electrodes based on timestamps from PC1 score\n",
    "val dataNoiseSegments = data\n",
    "    .join(noiseSegmentsTimeStamps, \"TimeStamp [µs]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 29:===================================================>  (191 + 8) / 200]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "folderName = #2_all_noise_segments\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "#2_all_noise_segments"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "///*\n",
    "// This is the saving of extracted noise segments.\n",
    "// The saved file is later imported into the notebook\n",
    "// \"Construct PSD tresholds from noise segment of each electrode (Python)\",\n",
    "// where the final noise tresholds are made.\n",
    "// saving noise segments to file v 2\n",
    "val folderName = experimentName + \"_all_noise_segments\"\n",
    "// now saving all noise segments as text file\n",
    "dataNoiseSegments.coalesce(1).write.csv(\"data/\" + folderName)\n",
    "//*/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
